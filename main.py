import os
import json
import requests
import yaml
import gzip
import logging
from datetime import datetime, timedelta
from pathlib import Path
from bs4 import BeautifulSoup

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_config(config_path='config.yaml'):
    with open(config_path) as f:
        return yaml.safe_load(f)

def process_sitemap(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        content = response.content
        # æ™ºèƒ½æ£€æµ‹gzipæ ¼å¼
        if content[:2] == b'\x1f\x8b':  # gzip magic number
            content = gzip.decompress(content)

        if b'<urlset' in content:
            return parse_xml(content)
        else:
            return parse_txt(content.decode('utf-8'))
    except requests.RequestException as e:
        logging.error(f"Error processing {url}: {str(e)}")
        return []
    except Exception as e:
        logging.error(f"Unexpected error processing {url}: {str(e)}")
        return []

def parse_xml(content):
    urls = []
    soup = BeautifulSoup(content, 'xml')
    for loc in soup.find_all('loc'):
        url = loc.get_text().strip()
        if url:
            urls.append(url)
    return urls

def parse_txt(content):
    return [line.strip() for line in content.splitlines() if line.strip()]

def save_latest(site_name, new_urls):
    base_dir = Path('latest')
    
    # åˆ›å»ºlatestç›®å½•ï¼ˆä¸æ—¥æœŸç›®å½•åŒçº§ï¼‰
    latest_dir = base_dir
    latest_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜latest.json
    latest_file = latest_dir / f'{site_name}.json'
    with open(latest_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(new_urls))

def save_diff(site_name, new_urls):
    base_dir = Path('diff')
        
    # åˆ›å»ºæ—¥æœŸç›®å½•
    today = datetime.now().strftime('%Y%m%d')
    date_dir = base_dir / today
    date_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å½“æ—¥æ–°å¢æ•°æ®
    file_path = date_dir / f'{site_name}.json'
    mode = 'a' if file_path.exists() else 'w'
    with open(file_path, mode, encoding='utf-8') as f:
        if mode == 'a':
            f.write('\n--------------------------------\n')  # æ·»åŠ åˆ†éš”ç¬¦
        f.write('\n'.join(new_urls) + '\n')  # ç¡®ä¿æ¯ä¸ªURLåéƒ½æœ‰æ¢è¡Œ

def compare_data(site_name, new_urls):
    latest_file = Path('latest') / f'{site_name}.json'
    
    if not latest_file.exists():
        return []
        
    with open(latest_file) as f:
        last_urls = set(f.read().splitlines())
    
    return [url for url in new_urls if url not in last_urls]

def send_feishu_notification(new_urls, config, site_name):
    if not new_urls:
        return
    
    webhook_url = config['feishu']['webhook_url']
    secret = config['feishu'].get('secret')
    
    message = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": f"ğŸ® {site_name} æ¸¸æˆä¸Šæ–°é€šçŸ¥"},
                "template": "green"
            },
            "elements": [
                {
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"**ä»Šæ—¥æ–°å¢ {len(new_urls)} æ¬¾æ¸¸æˆ**\n\n" + "\n".join(f"â€¢ {url}" for url in new_urls[:10])
                    }
                }
            ]
        }
    }
    
    for attempt in range(3):  # é‡è¯•æœºåˆ¶
        try:
            resp = requests.post(webhook_url, json=message)
            resp.raise_for_status()
            logging.info("é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
            return
        except requests.RequestException as e:
            logging.error(f"é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {str(e)}")
            if attempt < 2:
                logging.info("é‡è¯•å‘é€é€šçŸ¥...")

def main(config_path='config.yaml'):
    config = load_config(config_path)
    
    for site in config['sites']:
        if not site['active']:
            continue
            
        logging.info(f"å¤„ç†ç«™ç‚¹: {site['name']}")
        all_urls = []
        for sitemap_url in site['sitemap_urls']:
            urls = process_sitemap(sitemap_url)
            all_urls.extend(urls)
            
        # å»é‡å¤„ç†
        unique_urls = list({url: None for url in all_urls}.keys())
        new_urls = compare_data(site['name'], unique_urls)
        
        save_latest(site['name'], unique_urls)
        if new_urls:
            save_diff(site['name'], new_urls)
            send_feishu_notification(new_urls, config, site['name'])
            
        # æ¸…ç†æ—§æ•°æ®
        cleanup_old_data(site['name'], config)

def cleanup_old_data(site_name, config):
    data_dir = Path('diff')
    if not data_dir.exists():
        return
        
    # è·å–é…ç½®ä¸­çš„ä¿ç•™å¤©æ•°
    retention_days = config.get('retention_days', 7)
    cutoff = datetime.now() - timedelta(days=retention_days)
    
    # éå†æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
    for date_dir in data_dir.glob('*'):
        if not date_dir.is_dir():
            continue
            
        try:
            # è§£ææ–‡ä»¶å¤¹åç§°ä¸ºæ—¥æœŸ
            dir_date = datetime.strptime(date_dir.name, '%Y%m%d')
            if dir_date < cutoff:
                # åˆ é™¤æ•´ä¸ªæ—¥æœŸæ–‡ä»¶å¤¹
                for f in date_dir.glob('*.json'):
                    f.unlink()
                date_dir.rmdir()
                logging.info(f"å·²åˆ é™¤è¿‡æœŸæ–‡ä»¶å¤¹: {date_dir.name}")
        except ValueError:
            # å¿½ç•¥éæ—¥æœŸæ ¼å¼çš„æ–‡ä»¶å¤¹
            continue
        except Exception as e:
            logging.error(f"åˆ é™¤æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}")

if __name__ == '__main__':
    main()
